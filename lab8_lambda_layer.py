# -*- coding: utf-8 -*-
"""lab8_lambda_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/RobertNeat/656de14f1b0eae791261cd040613a3d0/lab8_lambda_layer.ipynb

# Neural network with a Lambda layers

The main objective for this code is to create neural network that uses lambda layers to build dense neural netowrk using functional interface.
"""

# Loading the data
from keras.datasets import mnist  # Importing the MNIST dataset from Keras
import numpy as np
import pandas as pd

data = mnist.load_data()  # Loading the MNIST dataset
X_train, y_train = data[0][0], data[0][1]  # Extracting training data and labels
X_test, y_test = data[1][0], data[1][1]  # Extracting test data and labels

# Expanding dimensions to match expected input shape
X_train = np.expand_dims(X_train, axis=-1)  # Adding an extra dimension to training data
X_test = np.expand_dims(X_test, axis=-1)  # Adding an extra dimension to test data

# Preprocessing labels using one-hot encoding
y_train = pd.get_dummies(pd.Categorical(y_train)).values  # One-hot encoding for training labels
y_test = pd.get_dummies(pd.Categorical(y_test)).values  # One-hot encoding for test labels

from keras.backend import softmax
from keras.layers import Conv2D, Flatten, Dense, Lambda, Input
import tensorflow as tf
from keras.models import Model

def mish(x):
  return x * tf.math.tanh(tf.math.softplus(x))

# Creating a simple neural network
output_tensor = input_tensor = Input(X_train.shape[1:])  # Creating input tensor based on the shape of X_train
output_tensor = Conv2D(32, (3, 3))(output_tensor)  # Adding a 2D convolutional layer with 32 filters of size 3x3
output_tensor = Lambda(mish)(output_tensor)  # Applying the Mish activation function
# output_tensor = AveragePooling2D()(output_tensor)  # To make plots prettier, add more layers here
output_tensor = Flatten()(output_tensor)  # Flattening the output tensor
output_tensor = Dense(10, activation='softmax')(output_tensor)  # Adding a Dense layer with 10 units and softmax activation
model4 = Model(inputs=input_tensor, outputs=output_tensor)  # Creating a model with input and output tensors
model4.compile(loss='categorical_crossentropy', metrics='accuracy', optimizer='adam')  # Compiling the model with categorical crossentropy loss, accuracy metrics, and Adam optimizer

# training model and testing on test batch
model4.fit(X_train,y_train,epochs=8,validation_data=(X_test,y_test))

# plotting the plots for loss and accuracy metrics
def generateNetworkLearningCurve(model,epchs=10):
  from matplotlib import pyplot as plt
  historia = model.history.history
  floss_train = historia['loss']
  floss_test = historia['val_loss']
  acc_train = historia['accuracy']
  acc_test = historia['val_accuracy']
  fig,ax = plt.subplots(1,2, figsize=(20,10))
  epochs = np.arange(0, epchs)
  ax[0].plot(epochs, floss_train, label='floss_train')
  ax[0].plot(epochs, floss_test, label='floss_test')
  ax[0].set_title('Funkcje strat')
  ax[0].legend()
  ax[1].set_title('Dokladnosci')
  ax[1].plot(epochs, acc_train, label='acc_train')
  ax[1].plot(epochs, acc_test, label='acc_test')
  ax[1].legend()

generateNetworkLearningCurve(model4,8)